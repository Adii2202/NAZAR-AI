{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2SoUsM-ryO3A"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l_vAfRwvyhJF"
      },
      "outputs": [],
      "source": [
        "FIRE_WEIGHTS = \"ML Models/Fire/fire.pt\"\n",
        "WEAPONS_WEIGHTS = \"ML Models/weapons detection/best.pt\"\n",
        "VIOLENCE_WEIGHTS = \"ML Models/Violence Detection/ViolenceDet.pt\"\n",
        "ABDUCTION_WEIGHTS = \"ML Models/Abduction detection/best (2).pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = {\n",
        "    \"generic\": [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
        "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
        "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
        "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
        "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
        "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
        "              ],\n",
        "    \"fire\" : [\"fire\",\"smoke\"],\n",
        "    \"violence\":[\"Violence\",\"Weapons\"],\n",
        "    \"weapons\" : [\"Grenade\",\"Handgun\",\"Rifle\",\"Steel arms\"],\n",
        "    \"abduction\":[\"Kidnap\", \"Non Kidnap\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {}\n",
        "models[\"generic\"] = YOLO(\"yolov8n.pt\")\n",
        "models[\"fire\"] = YOLO(FIRE_WEIGHTS)\n",
        "models[\"weapons\"] = YOLO(WEAPONS_WEIGHTS)\n",
        "models[\"violence\"] = YOLO(VIOLENCE_WEIGHTS)\n",
        "models[\"abduction\"] = YOLO(ABDUCTION_WEIGHTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pygame\n",
        "import matplotlib.pyplot as plt\n",
        "# Initialize pygame\n",
        "pygame.mixer.init()\n",
        "\n",
        "# Load siren sound\n",
        "siren_sound = pygame.mixer.Sound('police-6007.mp3')\n",
        "\n",
        "# Video capturing starts\n",
        "def tampering(frame):\n",
        "    # cap = cv2.VideoCapture(0)\n",
        "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
        "    # ret, frame = cap.read()\n",
        "    fgmask = fgbg.apply(frame)\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "    \n",
        "    if frame is None:\n",
        "        print(\"End of frame\")\n",
        "    else:\n",
        "        a = 0\n",
        "        bounding_rect = []\n",
        "        fgmask = fgbg.apply(frame)\n",
        "        fgmask = cv2.erode(fgmask, kernel, iterations=5) \n",
        "        fgmask = cv2.dilate(fgmask, kernel, iterations=5)\n",
        "        cv2.imshow('frame', frame)\n",
        "        contours, _ = cv2.findContours(fgmask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        for i in range(len(contours)):\n",
        "            bounding_rect.append(cv2.boundingRect(contours[i]))\n",
        "        for i in range(len(contours)):\n",
        "            if bounding_rect[i][2] >= 40 or bounding_rect[i][3] >= 40:\n",
        "                a = a + (bounding_rect[i][2]) * bounding_rect[i][3]\n",
        "            if a >= int(frame.shape[0]) * int(frame.shape[1]) / 3:\n",
        "                cv2.putText(frame, \"TAMPERING DETECTED\", (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "                # Play siren sound\n",
        "                siren_sound.play()\n",
        "            cv2.imshow('frame', frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "def create_csv():\n",
        "  try:\n",
        "    df = pd.read_csv('anomalies.csv')\n",
        "  except FileNotFoundError:\n",
        "    df = pd.DataFrame(columns=['Timestamp','Type of anomaly' ,'Number of People present', 'Path of image'])\n",
        "    df.to_csv('anomalies.csv', index=False)\n",
        "  return df\n",
        "\n",
        "def append_to_csv(df, timestamp, num_inmates, image_path, typeofanomaly=\"\"):\n",
        "  new_data = {'Timestamp': timestamp, 'Type of anomaly':typeofanomaly, 'Number of People present': num_inmates, 'Path of image': image_path}\n",
        "  new_data_df = pd.DataFrame.from_dict(new_data, orient='index').T\n",
        "  df = pd.concat([df, new_data_df], ignore_index=True)\n",
        "  df.to_csv('anomalies.csv', index=False)\n",
        "\n",
        "inmatesthresholdtime = 2\n",
        "anomaly_count = {} \n",
        "df = create_csv()\n",
        "\n",
        "def process_model(model, img, classNames, model_name, numinmates , savetodir=\"frames\"):\n",
        "    global inmatesthresholdtime, anomaly_count, df\n",
        "    \n",
        "    results = model(img, stream=True)\n",
        "    inmates = 0\n",
        "    anomalies = []\n",
        "    typeofanomaly = {\n",
        "            \"generic\": \"crowd\",\n",
        "            \"fire\": \"Fire\",\n",
        "            \"weapons\": \"Weapons\",\n",
        "            \"violence\": \"Violence\",\n",
        "            \"abduction\": \"Abduction\"\n",
        "        }\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            cls = int(box.cls[0])\n",
        "            confidence = math.ceil((box.conf[0]*100))/100\n",
        "            if confidence < 0.6:\n",
        "                continue\n",
        "            if cls >= len(classNames):\n",
        "                print(\"Error: Class index out of range\")\n",
        "                continue\n",
        "            accepted_list = [\"person\", \"fire\", \"Grenade\", \"Handgun\", \"Rifle\", \"Steel arms\",\"Violence\",\"Kidnap\", \"Not Kidnap\"]\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
        "            if model_name == \"generic\" and classNames[cls] == \"person\":\n",
        "                inmates += 1\n",
        "            if classNames[cls] in accepted_list:\n",
        "                anomaly_key = (model_name, classNames[cls])\n",
        "                anomaly_count[anomaly_key] = anomaly_count.get(anomaly_key, 0) + 1\n",
        "                if anomaly_count[anomaly_key] >= 1:\n",
        "                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S') \n",
        "                    anomaly_text = f\"{classNames[cls]} detected, storing the frame in the database as {savetodir}/{timestamp}.jpg at {timestamp}.\\n\"\n",
        "                    print(anomaly_text)\n",
        "                    anomalies.append(anomaly_text)\n",
        "                    anomaly_count[anomaly_key] = 0\n",
        "                    df = create_csv()\n",
        "                    if model_name == \"generic\" and inmates > numinmates:\n",
        "                      append_to_csv(df, timestamp, inmates, f\"{savetodir}/{timestamp}.jpg\", typeofanomaly[model_name])\n",
        "                    elif model_name != \"generic\":\n",
        "                      append_to_csv(df, timestamp, inmates, f\"{savetodir}/{timestamp}.jpg\", typeofanomaly[model_name])\n",
        "                    with open(\"anomalies.txt\", \"a\") as f:\n",
        "                        f.write(anomaly_text)\n",
        "                    # create the directory if it does not exist\n",
        "                    if not os.path.exists(savetodir):\n",
        "                        os.makedirs(savetodir)\n",
        "                    cv2.imwrite(filename=savetodir+\"/\"+timestamp+\".jpg\", img=img)\n",
        "                    \n",
        "                    \n",
        "            print(\"Confidence --->\", confidence)\n",
        "            print(\"Class name -->\", classNames[cls])\n",
        "            org = [x1, y1]\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            fontScale = 1\n",
        "            color = (255, 0, 0)\n",
        "            thickness = 2\n",
        "            cv2.putText(img, classNames[cls], org, font, fontScale, color, thickness)\n",
        "            \n",
        "    if inmatesthresholdtime <= 0:\n",
        "        inmatesthresholdtime = 2\n",
        "    else:\n",
        "        inmatesthresholdtime -= 1\n",
        "\n",
        "    with open(\"anomalies.txt\", \"a\") as f:\n",
        "        f.write(\" \".join(anomalies) )\n",
        "\n",
        "def start_webcam(models, classes):\n",
        "  cap = cv2.VideoCapture(0)\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      print(\"Error: Unable to capture frame\")\n",
        "      break\n",
        "    for model_name, model in models.items():\n",
        "      process_model(model, frame.copy(), classes[model_name], model_name, 10)  # Pass a copy of frame to avoid modification across models\n",
        "    cv2.imshow(\"frame\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run the function on demo video\n",
        "def start_demo(models, classes):\n",
        "    cap = cv2.VideoCapture(\"ML Models/weapons detection/gun.mp4\")\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        print(f\"{i}th frame\")\n",
        "        ret, frame = cap.read()\n",
        "        if i%10 != 0:\n",
        "            continue\n",
        "        if not ret:\n",
        "            print(\"Error: Unable to capture frame\")\n",
        "            break\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break \n",
        "        for model_name, model in models.items():\n",
        "            anomaly_detected = process_model(model, frame.copy(), classes[model_name], model_name, 10)  # Pass a copy of frame to avoid modification across models\n",
        "        if anomaly_detected:\n",
        "            try:\n",
        "                tampering(frame)\n",
        "            except Exception as e:\n",
        "                print(\"An error occurred while tampering the image:\", e)\n",
        "            cv2.imshow(\"frame\", frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "    \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1th frame\n",
            "2th frame\n",
            "3th frame\n",
            "4th frame\n",
            "5th frame\n",
            "6th frame\n",
            "7th frame\n",
            "8th frame\n",
            "9th frame\n",
            "10th frame\n",
            "\n",
            "0: 384x640 (no detections), 2193.0ms\n",
            "Speed: 83.7ms preprocess, 2193.0ms inference, 42.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1898.0ms\n",
            "Speed: 15.2ms preprocess, 1898.0ms inference, 5.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1962.0ms\n",
            "Speed: 28.7ms preprocess, 1962.0ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1116.3ms\n",
            "Speed: 4.5ms preprocess, 1116.3ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1349.7ms\n",
            "Speed: 9.5ms preprocess, 1349.7ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "11th frame\n",
            "12th frame\n",
            "13th frame\n",
            "14th frame\n",
            "15th frame\n",
            "16th frame\n",
            "17th frame\n",
            "18th frame\n",
            "19th frame\n",
            "20th frame\n",
            "\n",
            "0: 384x640 (no detections), 927.4ms\n",
            "Speed: 13.6ms preprocess, 927.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1893.9ms\n",
            "Speed: 10.3ms preprocess, 1893.9ms inference, 2.1ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1569.6ms\n",
            "Speed: 17.6ms preprocess, 1569.6ms inference, 23.2ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1043.1ms\n",
            "Speed: 11.1ms preprocess, 1043.1ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1423.2ms\n",
            "Speed: 2.0ms preprocess, 1423.2ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "21th frame\n",
            "22th frame\n",
            "23th frame\n",
            "24th frame\n",
            "25th frame\n",
            "26th frame\n",
            "27th frame\n",
            "28th frame\n",
            "29th frame\n",
            "30th frame\n",
            "\n",
            "0: 384x640 (no detections), 643.5ms\n",
            "Speed: 31.7ms preprocess, 643.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 2033.3ms\n",
            "Speed: 14.1ms preprocess, 2033.3ms inference, 3.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2017.3ms\n",
            "Speed: 11.6ms preprocess, 2017.3ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1599.5ms\n",
            "Speed: 13.6ms preprocess, 1599.5ms inference, 3.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1211.3ms\n",
            "Speed: 8.5ms preprocess, 1211.3ms inference, 2.1ms postprocess per image at shape (1, 3, 256, 448)\n",
            "31th frame\n",
            "32th frame\n",
            "33th frame\n",
            "34th frame\n",
            "35th frame\n",
            "36th frame\n",
            "37th frame\n",
            "38th frame\n",
            "39th frame\n",
            "40th frame\n",
            "\n",
            "0: 384x640 (no detections), 1183.5ms\n",
            "Speed: 4.5ms preprocess, 1183.5ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 720.4ms\n",
            "Speed: 10.5ms preprocess, 720.4ms inference, 3.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 621.0ms\n",
            "Speed: 6.0ms preprocess, 621.0ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1182.9ms\n",
            "Speed: 28.7ms preprocess, 1182.9ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 475.8ms\n",
            "Speed: 1.0ms preprocess, 475.8ms inference, 3.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "41th frame\n",
            "42th frame\n",
            "43th frame\n",
            "44th frame\n",
            "45th frame\n",
            "46th frame\n",
            "47th frame\n",
            "48th frame\n",
            "49th frame\n",
            "50th frame\n",
            "\n",
            "0: 384x640 (no detections), 375.6ms\n",
            "Speed: 3.0ms preprocess, 375.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1349.7ms\n",
            "Speed: 3.0ms preprocess, 1349.7ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1640.8ms\n",
            "Speed: 2.0ms preprocess, 1640.8ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1249.7ms\n",
            "Speed: 4.0ms preprocess, 1249.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1318.5ms\n",
            "Speed: 5.6ms preprocess, 1318.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "51th frame\n",
            "52th frame\n",
            "53th frame\n",
            "54th frame\n",
            "55th frame\n",
            "56th frame\n",
            "57th frame\n",
            "58th frame\n",
            "59th frame\n",
            "60th frame\n",
            "\n",
            "0: 384x640 (no detections), 742.3ms\n",
            "Speed: 26.6ms preprocess, 742.3ms inference, 36.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1472.3ms\n",
            "Speed: 2.0ms preprocess, 1472.3ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1543.6ms\n",
            "Speed: 4.0ms preprocess, 1543.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2088.5ms\n",
            "Speed: 14.1ms preprocess, 2088.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2585.5ms\n",
            "Speed: 4.0ms preprocess, 2585.5ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "61th frame\n",
            "62th frame\n",
            "63th frame\n",
            "64th frame\n",
            "65th frame\n",
            "66th frame\n",
            "67th frame\n",
            "68th frame\n",
            "69th frame\n",
            "70th frame\n",
            "\n",
            "0: 384x640 (no detections), 1252.4ms\n",
            "Speed: 5.0ms preprocess, 1252.4ms inference, 36.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1828.6ms\n",
            "Speed: 42.8ms preprocess, 1828.6ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 863.2ms\n",
            "Speed: 15.6ms preprocess, 863.2ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2252.3ms\n",
            "Speed: 5.5ms preprocess, 2252.3ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1617.4ms\n",
            "Speed: 3.0ms preprocess, 1617.4ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "71th frame\n",
            "72th frame\n",
            "73th frame\n",
            "74th frame\n",
            "75th frame\n",
            "76th frame\n",
            "77th frame\n",
            "78th frame\n",
            "79th frame\n",
            "80th frame\n",
            "\n",
            "0: 384x640 (no detections), 1047.1ms\n",
            "Speed: 6.5ms preprocess, 1047.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1047.8ms\n",
            "Speed: 2.5ms preprocess, 1047.8ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2154.2ms\n",
            "Speed: 4.9ms preprocess, 2154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2915.6ms\n",
            "Speed: 4.0ms preprocess, 2915.6ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2549.7ms\n",
            "Speed: 99.8ms preprocess, 2549.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "81th frame\n",
            "82th frame\n",
            "83th frame\n",
            "84th frame\n",
            "85th frame\n",
            "86th frame\n",
            "87th frame\n",
            "88th frame\n",
            "89th frame\n",
            "90th frame\n",
            "\n",
            "0: 384x640 (no detections), 934.2ms\n",
            "Speed: 44.7ms preprocess, 934.2ms inference, 22.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1400.0ms\n",
            "Speed: 8.6ms preprocess, 1400.0ms inference, 2.2ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1040.5ms\n",
            "Speed: 11.6ms preprocess, 1040.5ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1820.6ms\n",
            "Speed: 7.4ms preprocess, 1820.6ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1276.0ms\n",
            "Speed: 6.5ms preprocess, 1276.0ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "91th frame\n",
            "92th frame\n",
            "93th frame\n",
            "94th frame\n",
            "95th frame\n",
            "96th frame\n",
            "97th frame\n",
            "98th frame\n",
            "99th frame\n",
            "100th frame\n",
            "\n",
            "0: 384x640 (no detections), 855.6ms\n",
            "Speed: 11.6ms preprocess, 855.6ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1956.6ms\n",
            "Speed: 5.6ms preprocess, 1956.6ms inference, 11.6ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1772.1ms\n",
            "Speed: 2.5ms preprocess, 1772.1ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1115.1ms\n",
            "Speed: 44.8ms preprocess, 1115.1ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1900.9ms\n",
            "Speed: 31.2ms preprocess, 1900.9ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "101th frame\n",
            "102th frame\n",
            "103th frame\n",
            "104th frame\n",
            "105th frame\n",
            "106th frame\n",
            "107th frame\n",
            "108th frame\n",
            "109th frame\n",
            "110th frame\n",
            "\n",
            "0: 384x640 (no detections), 932.6ms\n",
            "Speed: 6.0ms preprocess, 932.6ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1666.5ms\n",
            "Speed: 60.4ms preprocess, 1666.5ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1621.4ms\n",
            "Speed: 4.0ms preprocess, 1621.4ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2624.5ms\n",
            "Speed: 6.5ms preprocess, 2624.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2069.8ms\n",
            "Speed: 4.5ms preprocess, 2069.8ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "111th frame\n",
            "112th frame\n",
            "113th frame\n",
            "114th frame\n",
            "115th frame\n",
            "116th frame\n",
            "117th frame\n",
            "118th frame\n",
            "119th frame\n",
            "120th frame\n",
            "\n",
            "0: 384x640 (no detections), 685.7ms\n",
            "Speed: 14.1ms preprocess, 685.7ms inference, 13.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1084.0ms\n",
            "Speed: 2.0ms preprocess, 1084.0ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1105.0ms\n",
            "Speed: 5.1ms preprocess, 1105.0ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2629.4ms\n",
            "Speed: 3.5ms preprocess, 2629.4ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 3270.6ms\n",
            "Speed: 6.5ms preprocess, 3270.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "121th frame\n",
            "122th frame\n",
            "123th frame\n",
            "124th frame\n",
            "125th frame\n",
            "126th frame\n",
            "127th frame\n",
            "128th frame\n",
            "129th frame\n",
            "130th frame\n",
            "\n",
            "0: 384x640 (no detections), 2559.3ms\n",
            "Speed: 34.6ms preprocess, 2559.3ms inference, 15.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 3740.3ms\n",
            "Speed: 108.6ms preprocess, 3740.3ms inference, 1.9ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1299.2ms\n",
            "Speed: 37.2ms preprocess, 1299.2ms inference, 4.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 3, 1464.6ms\n",
            "Speed: 20.4ms preprocess, 1464.6ms inference, 108.4ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 2105.6ms\n",
            "Speed: 10.5ms preprocess, 2105.6ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "131th frame\n",
            "132th frame\n",
            "133th frame\n",
            "134th frame\n",
            "135th frame\n",
            "136th frame\n",
            "137th frame\n",
            "138th frame\n",
            "139th frame\n",
            "140th frame\n",
            "\n",
            "0: 384x640 1 remote, 1 refrigerator, 779.9ms\n",
            "Speed: 11.7ms preprocess, 779.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1568.2ms\n",
            "Speed: 8.1ms preprocess, 1568.2ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1385.3ms\n",
            "Speed: 12.5ms preprocess, 1385.3ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 3, 614.1ms\n",
            "Speed: 3.0ms preprocess, 614.1ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 599.7ms\n",
            "Speed: 9.5ms preprocess, 599.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "141th frame\n",
            "142th frame\n",
            "143th frame\n",
            "144th frame\n",
            "145th frame\n",
            "146th frame\n",
            "147th frame\n",
            "148th frame\n",
            "149th frame\n",
            "150th frame\n",
            "\n",
            "0: 384x640 1 remote, 1 refrigerator, 449.4ms\n",
            "Speed: 4.0ms preprocess, 449.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1272.3ms\n",
            "Speed: 2.0ms preprocess, 1272.3ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1000.5ms\n",
            "Speed: 4.6ms preprocess, 1000.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1048.6ms\n",
            "Speed: 82.8ms preprocess, 1048.6ms inference, 3.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 992.3ms\n",
            "Speed: 6.0ms preprocess, 992.3ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "151th frame\n",
            "152th frame\n",
            "153th frame\n",
            "154th frame\n",
            "155th frame\n",
            "156th frame\n",
            "157th frame\n",
            "158th frame\n",
            "159th frame\n",
            "160th frame\n",
            "\n",
            "0: 384x640 1 person, 1 refrigerator, 468.0ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:10:59.jpg at 2024-03-14 08:10:59.\n",
            "\n",
            "Confidence ---> 0.74\n",
            "Class name --> person\n",
            "Speed: 22.2ms preprocess, 468.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1115.7ms\n",
            "Speed: 21.2ms preprocess, 1115.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1371.7ms\n",
            "Speed: 10.6ms preprocess, 1371.7ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 604.6ms\n",
            "Speed: 5.5ms preprocess, 604.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 788.1ms\n",
            "Speed: 4.0ms preprocess, 788.1ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "161th frame\n",
            "162th frame\n",
            "163th frame\n",
            "164th frame\n",
            "165th frame\n",
            "166th frame\n",
            "167th frame\n",
            "168th frame\n",
            "169th frame\n",
            "170th frame\n",
            "\n",
            "0: 384x640 1 person, 330.5ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:04.jpg at 2024-03-14 08:11:04.\n",
            "\n",
            "Confidence ---> 0.81\n",
            "Class name --> person\n",
            "Speed: 9.6ms preprocess, 330.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 890.7ms\n",
            "Speed: 3.0ms preprocess, 890.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 2 Steel armss, 1276.2ms\n",
            "Speed: 4.0ms preprocess, 1276.2ms inference, 15.3ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Violence - Weapon Detection - v1 2023-09-11 8-03pm, 1702.7ms\n",
            "Speed: 8.5ms preprocess, 1702.7ms inference, 4.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 1147.6ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:09.jpg at 2024-03-14 08:11:09.\n",
            "\n",
            "Confidence ---> 0.73\n",
            "Class name --> Kidnap\n",
            "Speed: 38.2ms preprocess, 1147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "171th frame\n",
            "172th frame\n",
            "173th frame\n",
            "174th frame\n",
            "175th frame\n",
            "176th frame\n",
            "177th frame\n",
            "178th frame\n",
            "179th frame\n",
            "180th frame\n",
            "\n",
            "0: 384x640 1 person, 783.4ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:11.jpg at 2024-03-14 08:11:11.\n",
            "\n",
            "Confidence ---> 0.82\n",
            "Class name --> person\n",
            "Speed: 4.0ms preprocess, 783.4ms inference, 25.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1610.1ms\n",
            "Speed: 7.0ms preprocess, 1610.1ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1438.2ms\n",
            "Speed: 32.1ms preprocess, 1438.2ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Violence - Weapon Detection - v1 2023-09-11 8-03pm, 1356.1ms\n",
            "Speed: 26.1ms preprocess, 1356.1ms inference, 6.6ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 1326.7ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:17.jpg at 2024-03-14 08:11:17.\n",
            "\n",
            "Confidence ---> 0.75\n",
            "Class name --> Kidnap\n",
            "Speed: 3.0ms preprocess, 1326.7ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "181th frame\n",
            "182th frame\n",
            "183th frame\n",
            "184th frame\n",
            "185th frame\n",
            "186th frame\n",
            "187th frame\n",
            "188th frame\n",
            "189th frame\n",
            "190th frame\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 1637.2ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:19.jpg at 2024-03-14 08:11:19.\n",
            "\n",
            "Confidence ---> 0.8\n",
            "Class name --> person\n",
            "Speed: 20.6ms preprocess, 1637.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 2390.2ms\n",
            "Speed: 46.1ms preprocess, 2390.2ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1382.7ms\n",
            "Speed: 5.0ms preprocess, 1382.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Violence - Weapon Detection - v1 2023-09-11 8-03pm, 1069.9ms\n",
            "Error: Class index out of range\n",
            "Speed: 6.5ms preprocess, 1069.9ms inference, 4.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 908.1ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:25.jpg at 2024-03-14 08:11:25.\n",
            "\n",
            "Confidence ---> 0.76\n",
            "Class name --> Kidnap\n",
            "Speed: 2.5ms preprocess, 908.1ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "191th frame\n",
            "192th frame\n",
            "193th frame\n",
            "194th frame\n",
            "195th frame\n",
            "196th frame\n",
            "197th frame\n",
            "198th frame\n",
            "199th frame\n",
            "200th frame\n",
            "\n",
            "0: 384x640 1 person, 330.8ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:25.jpg at 2024-03-14 08:11:25.\n",
            "\n",
            "Confidence ---> 0.9\n",
            "Class name --> person\n",
            "Speed: 8.6ms preprocess, 330.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1144.9ms\n",
            "Speed: 8.5ms preprocess, 1144.9ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Steel arms, 980.5ms\n",
            "Speed: 3.5ms preprocess, 980.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Violence - Weapon Detection - v1 2023-09-11 8-03pm, 765.8ms\n",
            "Speed: 2.0ms preprocess, 765.8ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 1138.3ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:29.jpg at 2024-03-14 08:11:29.\n",
            "\n",
            "Confidence ---> 0.7\n",
            "Class name --> Kidnap\n",
            "Speed: 7.5ms preprocess, 1138.3ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "201th frame\n",
            "202th frame\n",
            "203th frame\n",
            "204th frame\n",
            "205th frame\n",
            "206th frame\n",
            "207th frame\n",
            "208th frame\n",
            "209th frame\n",
            "210th frame\n",
            "\n",
            "0: 384x640 1 person, 1 tv, 695.6ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:31.jpg at 2024-03-14 08:11:31.\n",
            "\n",
            "Confidence ---> 0.9\n",
            "Class name --> person\n",
            "Speed: 15.1ms preprocess, 695.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1760.9ms\n",
            "Speed: 2.0ms preprocess, 1760.9ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Rifle, 1219.2ms\n",
            "Speed: 1.5ms preprocess, 1219.2ms inference, 4.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Violence - Weapon Detection - v1 2023-09-11 8-03pm, 1062.3ms\n",
            "Speed: 4.0ms preprocess, 1062.3ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 2 Kidnaps, 1131.3ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:36.jpg at 2024-03-14 08:11:36.\n",
            "\n",
            "Confidence ---> 0.78\n",
            "Class name --> Kidnap\n",
            "Speed: 18.6ms preprocess, 1131.3ms inference, 13.1ms postprocess per image at shape (1, 3, 256, 448)\n",
            "211th frame\n",
            "212th frame\n",
            "213th frame\n",
            "214th frame\n",
            "215th frame\n",
            "216th frame\n",
            "217th frame\n",
            "218th frame\n",
            "219th frame\n",
            "220th frame\n",
            "\n",
            "0: 384x640 1 person, 746.9ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:37.jpg at 2024-03-14 08:11:37.\n",
            "\n",
            "Confidence ---> 0.87\n",
            "Class name --> person\n",
            "Speed: 7.5ms preprocess, 746.9ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1600.5ms\n",
            "Speed: 6.0ms preprocess, 1600.5ms inference, 2.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1443.2ms\n",
            "Speed: 18.1ms preprocess, 1443.2ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1376.7ms\n",
            "Speed: 32.6ms preprocess, 1376.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 442.7ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:42.jpg at 2024-03-14 08:11:42.\n",
            "\n",
            "Confidence ---> 0.75\n",
            "Class name --> Kidnap\n",
            "Speed: 2.0ms preprocess, 442.7ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "221th frame\n",
            "222th frame\n",
            "223th frame\n",
            "224th frame\n",
            "225th frame\n",
            "226th frame\n",
            "227th frame\n",
            "228th frame\n",
            "229th frame\n",
            "230th frame\n",
            "\n",
            "0: 384x640 1 person, 107.5ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:44.jpg at 2024-03-14 08:11:44.\n",
            "\n",
            "Confidence ---> 0.87\n",
            "Class name --> person\n",
            "Speed: 390.5ms preprocess, 107.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 427.0ms\n",
            "Speed: 13.1ms preprocess, 427.0ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 285.5ms\n",
            "Speed: 2.0ms preprocess, 285.5ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 393.9ms\n",
            "Speed: 1.5ms preprocess, 393.9ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 498.8ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:46.jpg at 2024-03-14 08:11:46.\n",
            "\n",
            "Confidence ---> 0.75\n",
            "Class name --> Kidnap\n",
            "Speed: 1.0ms preprocess, 498.8ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "231th frame\n",
            "232th frame\n",
            "233th frame\n",
            "234th frame\n",
            "235th frame\n",
            "236th frame\n",
            "237th frame\n",
            "238th frame\n",
            "239th frame\n",
            "240th frame\n",
            "\n",
            "0: 384x640 1 person, 244.0ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:47.jpg at 2024-03-14 08:11:47.\n",
            "\n",
            "Confidence ---> 0.87\n",
            "Class name --> person\n",
            "Speed: 4.0ms preprocess, 244.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 581.4ms\n",
            "Speed: 5.5ms preprocess, 581.4ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 414.1ms\n",
            "Speed: 3.9ms preprocess, 414.1ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 457.2ms\n",
            "Speed: 3.1ms preprocess, 457.2ms inference, 0.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 335.6ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:49.jpg at 2024-03-14 08:11:49.\n",
            "\n",
            "Confidence ---> 0.75\n",
            "Class name --> Kidnap\n",
            "Speed: 3.0ms preprocess, 335.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "241th frame\n",
            "242th frame\n",
            "243th frame\n",
            "244th frame\n",
            "245th frame\n",
            "246th frame\n",
            "247th frame\n",
            "248th frame\n",
            "249th frame\n",
            "250th frame\n",
            "\n",
            "0: 384x640 1 person, 423.3ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:50.jpg at 2024-03-14 08:11:50.\n",
            "\n",
            "Confidence ---> 0.85\n",
            "Class name --> person\n",
            "Speed: 21.4ms preprocess, 423.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 1277.1ms\n",
            "Speed: 6.0ms preprocess, 1277.1ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Steel arms, 1920.2ms\n",
            "Speed: 61.1ms preprocess, 1920.2ms inference, 9.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 1729.8ms\n",
            "Speed: 18.1ms preprocess, 1729.8ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 1 Kidnap, 1291.6ms\n",
            "Kidnap detected, storing the frame in the database as frames/2024-03-14 08:11:56.jpg at 2024-03-14 08:11:56.\n",
            "\n",
            "Confidence ---> 0.78\n",
            "Class name --> Kidnap\n",
            "Speed: 4.6ms preprocess, 1291.6ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "251th frame\n",
            "252th frame\n",
            "253th frame\n",
            "254th frame\n",
            "255th frame\n",
            "256th frame\n",
            "257th frame\n",
            "258th frame\n",
            "259th frame\n",
            "260th frame\n",
            "\n",
            "0: 384x640 1 person, 1 car, 446.8ms\n",
            "person detected, storing the frame in the database as frames/2024-03-14 08:11:57.jpg at 2024-03-14 08:11:57.\n",
            "\n",
            "Confidence ---> 0.8\n",
            "Class name --> person\n",
            "Confidence ---> 0.62\n",
            "Class name --> car\n",
            "Speed: 27.2ms preprocess, 446.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 256x448 (no detections), 2094.1ms\n",
            "Speed: 7.2ms preprocess, 2094.1ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 2 Handguns, 1159.8ms\n",
            "Speed: 3.5ms preprocess, 1159.8ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n",
            "0: 256x448 (no detections), 2351.5ms\n",
            "Speed: 51.3ms preprocess, 2351.5ms inference, 3.5ms postprocess per image at shape (1, 3, 256, 448)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstart_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mstart_demo\u001b[1;34m(models, classes)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 17\u001b[0m     anomaly_detected \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass a copy of frame to avoid modification across models\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m anomaly_detected:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "Cell \u001b[1;32mIn[10], line 34\u001b[0m, in \u001b[0;36mprocess_model\u001b[1;34m(model, img, classNames, model_name, numinmates, savetodir)\u001b[0m\n\u001b[0;32m     26\u001b[0m anomalies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m typeofanomaly \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfire\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFire\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabduction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbduction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m     }\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     35\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mboxes\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m boxes:\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:425\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 425\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:231\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 231\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:231\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 231\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:341\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_demo(models, classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_webcam(models=models, classes=classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lida\n",
        "from lida import llm, Manager, TextGenerationConfig\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = \"hk305qjjFaCIXME1dGS0OIIHDth4NFnTtuzpPCOg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your data (replace with your data source)\n",
        "data = pd.read_csv(\"anomalies.csv\")\n",
        "\n",
        "lida = Manager(text_gen = llm(\"cohere\")) \n",
        "textgen_config = TextGenerationConfig(n=3, temperature=0.5, use_cache=True)\n",
        "\n",
        "try:\n",
        "    summary = lida.summarize(data=data, summary_method=\"default\", textgen_config=textgen_config)  \n",
        "    goals = lida.goals(summary, n=2, textgen_config=textgen_config)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "try: \n",
        "    for goal in goals:\n",
        "        display(goal)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# textgen_config = TextGenerationConfig(n=1, temperature=0.2)\n",
        "# persona = \"I am a security guard at a prison. I want to be able to detect anomalies in the prison environment.\"\n",
        "# personal_goals = lida.goals(summary, n=2, persona=persona, textgen_config=textgen_config)\n",
        "# for goal in personal_goals:\n",
        "#     display(goal)\n",
        "\n",
        "i = 0\n",
        "library = \"matplotlib\"\n",
        "\n",
        "charts = lida.visualize(summary=summary, goal=goal, textgen_config=textgen_config, library=library)  \n",
        "\n",
        "for chart in charts:\n",
        "    try:\n",
        "        chart.show()\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms.huggingface_hub import HuggingFaceHub\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "import os\n",
        "from dotenv import load_dotenv, get_key\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = get_key(key_to_get=\"HUGGINGFACEHUB_API_KEY\",dotenv_path=\".env\")\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.5,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")\n",
        "\n",
        "def chatwithbot(txt:str):\n",
        "    prompt = PromptTemplate(template= \"You're a helpful security assistant. You've been asked to help authorities. They want to be able to detect securty concerns in the city. They've provided you with a list of anomalies in the city with type of anomaly and timestamp.Answer questions of user from given data. Here's the data:\\n{data}\", input_variables=[\"data\"])\n",
        "\n",
        "    chat_model = ChatHuggingFace(llm=llm)\n",
        "    # get the data from anomalies.txt\n",
        "    with open(\"anomalies.txt\", \"r\") as f:\n",
        "        data = f.read()\n",
        "    if len(data) == 0:\n",
        "        data = \"No anomalies detected\"\n",
        "\n",
        "    final_prompt = prompt.format(data=data)\n",
        "    user_template= PromptTemplate(template=\"{user_input}\", input_variables=[\"user_input\"])\n",
        "\n",
        "    messages = [\n",
        "    SystemMessage(content=final_prompt),\n",
        "    HumanMessage(content=user_template.format(user_input=txt))\n",
        "    ]\n",
        "    print(chat_model(messages))\n",
        "    res = chat_model(messages).content\n",
        "    res = res[res.find(\"<|assistant|>\")+len(\"<|assistant|>\"):]\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def chat():\n",
        "    while True:\n",
        "        txt = input(\"Enter your message: \")\n",
        "        if txt == \"exit\":\n",
        "            break\n",
        "        res = chatwithbot(txt)\n",
        "        print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
